"""
DocumentGPT Dev Handler - LangGraph Orchestration + MCP-style Tooling
"""
import base64
import json
import mimetypes
import os
import traceback
from datetime import datetime
from decimal import Decimal
from typing import Optional

import boto3

import requests
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.tools import Tool
from langchain_core.messages import HumanMessage, SystemMessage

from agents import DEFAULT_RESEARCH_SYSTEM_PROMPT, build_langgraph_agent, web_search
from config import get_settings, make_cors_headers

# Environment
settings = get_settings()
OPENAI_API_KEY = settings.openai_api_key
PINECONE_API_KEY = settings.pinecone_api_key
PINECONE_INDEX_NAME = settings.pinecone_index or 'documentgpt-dev'
PINECONE_INDEX_HOST = settings.pinecone_index_host
DOC_TABLE = settings.doc_table
MEDIA_BUCKET = settings.media_bucket
MEDIA_QUEUE_URL = settings.media_queue_url

# Ensure Pinecone cache can write inside Lambda /tmp filesystem
os.environ["HOME"] = "/tmp"
os.environ["PINECONE_CACHE_DIR"] = "/tmp/pinecone"
os.environ["XDG_CACHE_HOME"] = "/tmp/.cache"
os.environ["XDG_CONFIG_HOME"] = "/tmp/.config"
os.makedirs("/tmp/.cache", exist_ok=True)
os.makedirs("/tmp/.config", exist_ok=True)
os.makedirs(os.environ["PINECONE_CACHE_DIR"], exist_ok=True)

# AWS clients
dynamodb = boto3.resource('dynamodb')
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# LangChain setup
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, openai_api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)

# Pinecone REST helpers
def pinecone_request(path, payload):
    if not PINECONE_INDEX_HOST:
        raise RuntimeError("PINECONE_INDEX_HOST not configured")

    url = f"https://{PINECONE_INDEX_HOST}{path}"
    headers = {
        "Content-Type": "application/json",
        "Api-Key": PINECONE_API_KEY,
    }
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
    except requests.RequestException as request_error:
        raise RuntimeError(f"Pinecone request failed: {request_error}") from request_error

    if not response.ok:
        raise RuntimeError(
            f"Pinecone request failed ({response.status_code}): {response.text[:300]}"
        )
    return response.json()


def pinecone_upsert(vectors):
    if not vectors:
        return

    batch_size = 100
    for i in range(0, len(vectors), batch_size):
        batch = {"vectors": vectors[i : i + batch_size]}
        pinecone_request("/vectors/upsert", batch)


def pinecone_query(vector, doc_id=None, top_k=5):
    body = {
        "vector": vector,
        "topK": top_k,
        "includeMetadata": True,
    }
    if doc_id:
        body["filter"] = {"doc_id": {"$eq": doc_id}}
    data = pinecone_request("/query", body)
    return data.get("matches", [])

# Text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

class DecimalEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, Decimal):
            return int(obj)
        return super().default(obj)

def make_headers(content_type='application/json', request_headers=None):
    return make_cors_headers(
        settings,
        request_headers=request_headers,
        content_type=content_type,
        add_origin_header=True,
    )

# MCP-style Tools
def pinecone_retrieve(query: str, doc_id: str = None) -> str:
    """Retrieve relevant document chunks from Pinecone vector database"""
    try:
        query_embedding = embeddings.embed_query(query)
        results = pinecone_query(query_embedding, doc_id=doc_id, top_k=5)

        if not results:
            return "No relevant passages found in documents."

        passages = []
        for idx, match in enumerate(results):
            metadata = match.get("metadata") or {}
            text = metadata.get("text") or ""
            if not text:
                continue
            passages.append(f"[{idx + 1}] {text}")

        if not passages:
            return "No relevant passages found in documents."

        context = "\n\n".join(passages)
        return f"RELEVANT PASSAGES:\n{context}"
    except Exception as e:
        print(f"‚ö†Ô∏è Pinecone retrieve error: {e}")
        traceback.print_exc()
        return "Error retrieving from vector database."

# Define tools (mutable list so we can swap document filter per-request)
tools = [
    Tool(
        name="document_search",
        func=lambda q: pinecone_retrieve(q),
        description="Search user's uploaded documents for relevant information. Use this FIRST for any question about documents.",
    ),
    Tool(
        name="web_search",
        func=web_search,
        description="Search the web for current information or facts not in documents. Use ONLY if document_search returns no results.",
    ),
]

RESEARCH_SYSTEM_PROMPT = DEFAULT_RESEARCH_SYSTEM_PROMPT

research_agent = build_langgraph_agent(llm, RESEARCH_SYSTEM_PROMPT, tools)

def extract_pdf_text(content):
    """Extract text from PDF content"""
    try:
        import PyPDF2
        import io
        pdf_file = io.BytesIO(content.encode('latin-1') if isinstance(content, str) else content)
        reader = PyPDF2.PdfReader(pdf_file)
        return "\n".join([page.extract_text() for page in reader.pages])
    except Exception as e:
        print(f"‚ö†Ô∏è PDF extraction failed: {e}")
        return content

def generate_summary(text, doc_name):
    """Generate document summary using LLM"""
    try:
        prompt = f"Summarize this document in 3-5 sentences:\n\n{text[:8000]}"
        response = llm.invoke(prompt)
        return response.content
    except Exception as e:
        print(f"‚ö†Ô∏è Summary generation failed: {e}")
        return f"Document {doc_name} uploaded successfully."

def lambda_handler(event, context):
    """Main Lambda handler"""
    request_headers = event.get('headers', {})
    headers = make_headers(request_headers=request_headers)
    
    try:
        # Parse request
        if 'requestContext' in event and 'http' in event['requestContext']:
            method = event['requestContext']['http']['method']
            path = event.get('rawPath', '')
        else:
            method = event.get('httpMethod', '')
            path = event.get('path', '')
        
        print(f"üìç {method} {path}")
        
        # OPTIONS
        if method == 'OPTIONS':
            return {'statusCode': 200, 'headers': headers, 'body': ''}
        
        # Health check
        if path == '/dev/health' and method == 'GET':
            return {
                'statusCode': 200,
                'headers': headers,
                'body': json.dumps({
                    'status': 'healthy',
                    'environment': 'dev',
                    'langchain': True,
                    'mcp_enabled': True,
                    'timestamp': datetime.now().isoformat()
                })
            }
        
        # Upload endpoint
        if path in ('/dev/upload', '/upload') and method == 'POST':
            body = json.loads(event['body'])
            user_id = body.get('user_id', 'guest_dev')
            filename = body.get('filename')
            content = body.get('content')
            content_base64 = body.get('content_base64')
            media_type = body.get('media_type')

            if not filename:
                return {
                    'statusCode': 400,
                    'headers': headers,
                    'body': json.dumps({'error': 'Missing filename'})
                }

            if not content and not content_base64:
                return {
                    'statusCode': 400,
                    'headers': headers,
                    'body': json.dumps({'error': 'Missing content'})
                }

            doc_id = f"doc_{int(datetime.now().timestamp())}"
            print(f"üìÑ Processing: {filename}")

            extension = os.path.splitext(filename)[1].lower()
            guessed_type, _ = mimetypes.guess_type(filename)
            media_type = media_type or guessed_type or 'application/octet-stream'

            is_text_like = media_type.startswith('text/') or extension in {'.txt', '.md', '.markdown', '.csv'}
            is_pdf = extension == '.pdf' or media_type == 'application/pdf'

            binary_payload: Optional[bytes] = None

            if content_base64:
                try:
                    binary_payload = base64.b64decode(content_base64)
                except Exception:
                    return {
                        'statusCode': 400,
                        'headers': headers,
                        'body': json.dumps({'error': 'Invalid base64 payload'})
                    }

            if binary_payload and not (is_text_like or is_pdf):
                if not MEDIA_BUCKET or not MEDIA_QUEUE_URL:
                    return {
                        'statusCode': 500,
                        'headers': headers,
                        'body': json.dumps({'error': 'Media processing not configured'})
                    }

                s3_key = f"uploads/{user_id}/{doc_id}/{filename}"
                s3.put_object(
                    Bucket=MEDIA_BUCKET,
                    Key=s3_key,
                    Body=binary_payload,
                    ContentType=media_type,
                )
                print(f"‚òÅÔ∏è  Stored media in S3 at {s3_key}")

                docs_table = dynamodb.Table(DOC_TABLE)
                docs_table.put_item(Item={
                    'pk': f'USER#{user_id}',
                    'sk': f'DOC#{doc_id}',
                    'doc_id': doc_id,
                    'filename': filename,
                    'media_type': media_type,
                    'processing_status': 'processing',
                    'created_at': datetime.now().isoformat()
                })

                job_payload = {
                    'user_id': user_id,
                    'doc_id': doc_id,
                    'bucket': MEDIA_BUCKET,
                    'key': s3_key,
                    'media_type': media_type,
                    'metadata': body.get('metadata') or {},
                    'segments': body.get('segments') or [],
                }
                sqs.send_message(QueueUrl=MEDIA_QUEUE_URL, MessageBody=json.dumps(job_payload))
                print(f"üì¨ Enqueued media processing job for {doc_id}")

                return {
                    'statusCode': 202,
                    'headers': headers,
                    'body': json.dumps({
                        'message': 'Media received and queued for processing',
                        'doc_id': doc_id,
                        'processing_status': 'processing'
                    })
                }

            if binary_payload and (is_text_like or is_pdf):
                if is_pdf:
                    content = extract_pdf_text(binary_payload)
                else:
                    content = binary_payload.decode('utf-8', errors='ignore')

            if is_pdf and not binary_payload:
                content = extract_pdf_text(content)

            if not content:
                return {
                    'statusCode': 400,
                    'headers': headers,
                    'body': json.dumps({'error': 'Unable to process document content'})
                }

            chunks = text_splitter.split_text(content)
            print(f"‚úÇÔ∏è  Split into {len(chunks)} chunks")

            print("üîß Preparing Pinecone payload", flush=True)
            try:
                embeddings_list = embeddings.embed_documents(chunks)
            except Exception as embed_error:
                print(f"‚ùå Embedding error: {embed_error!r}")
                traceback.print_exc()
                raise

            print("üìå Upserting embeddings to Pinecone", flush=True)
            vectors = []
            for idx, (chunk, vector) in enumerate(zip(chunks, embeddings_list)):
                vectors.append(
                    {
                        "id": f"{doc_id}-{idx}",
                        "values": vector,
                        "metadata": {
                            "doc_id": doc_id,
                            "doc_name": filename,
                            "chunk": idx,
                            "text": chunk,
                            "user_id": user_id,
                        },
                    }
                )

            try:
                pinecone_upsert(vectors)
            except Exception as pinecone_error:
                print(f"‚ùå Pinecone upsert error: {pinecone_error!r}")
                traceback.print_exc()
                raise
            print("‚úÖ Vectorized and stored in Pinecone")

            print("üß† Generating summary", flush=True)
            summary = generate_summary(content, filename)
            print("üß† Summary generated", flush=True)

            questions = [
                f"What are the main topics in {filename}?",
                "Can you summarize the key findings?",
                "What are the most important points?"
            ]

            docs_table = dynamodb.Table(DOC_TABLE)
            print("üóÑÔ∏è  Writing document metadata to DynamoDB", flush=True)
            docs_table.put_item(Item={
                'pk': f'USER#{user_id}',
                'sk': f'DOC#{doc_id}',
                'doc_id': doc_id,
                'filename': filename,
                'media_type': media_type,
                'content': content[:50000],
                'summary': summary,
                'questions': questions,
                'processing_status': 'ready',
                'created_at': datetime.now().isoformat()
            })
            print("üóÑÔ∏è  DynamoDB write complete", flush=True)

            return {
                'statusCode': 200,
                'headers': headers,
                'body': json.dumps({
                    'message': 'Document uploaded',
                    'doc_id': doc_id,
                    'artifact': {
                        'summary': summary,
                        'questions': questions
                    }
                })
            }
        
        # Chat endpoint with LangChain agent
        if path == '/dev/chat' and method == 'POST':
            body = json.loads(event['body'])
            query = body.get('query') or body.get('messages', [{}])[-1].get('content', '')
            doc_id = body.get('doc_id') or body.get('documentId')
            
            if not query:
                return {
                    'statusCode': 400,
                    'headers': headers,
                    'body': json.dumps({'error': 'No query provided'})
                }
            
            print(f"üí¨ Query: {query[:100]}")
            
            # Modify pinecone_retrieve to use doc_id if provided
            original_func = tools[0].func
            if doc_id:
                tools[0].func = lambda q, doc_id=doc_id: pinecone_retrieve(q, doc_id)
            
            # Run agent
            try:
                response_text, citations, tool_traces = research_agent(query)
                return {
                    'statusCode': 200,
                    'headers': headers,
                    'body': json.dumps({
                        'response': response_text,
                        'citations': citations,
                        'tool_traces': tool_traces
                    })
                }
            except Exception as e:
                print(f"‚ùå Agent error: {e}")
                return {
                    'statusCode': 500,
                    'headers': headers,
                    'body': json.dumps({
                        'response': 'Sorry, I encountered an error processing your request.',
                        'error': str(e)
                    })
                }
            finally:
                tools[0].func = original_func

        if path == '/dev/autocomplete' and method == 'POST':
            try:
                body = json.loads(event.get('body') or '{}')
            except json.JSONDecodeError:
                return {
                    'statusCode': 400,
                    'headers': headers,
                    'body': json.dumps({'error': 'Invalid JSON body'})
                }

            context = (body.get('context') or '').strip()
            max_tokens = int(body.get('max_tokens', 30))
            style = (body.get('style') or '').lower().strip()

            if len(context) < 20:
                return {
                    'statusCode': 400,
                    'headers': headers,
                    'body': json.dumps({'error': 'Context too short'})
                }

            max_tokens = max(10, min(max_tokens, 80))

            style_instructions = {
                'hemingway': "Write with short, vivid sentences and concrete imagery, reminiscent of Ernest Hemingway.",
                'academic': "Adopt a formal, academic tone with precise language and clear argumentation.",
                'casual': "Use a relaxed, conversational tone as if speaking with a friend.",
                'storyteller': "Continue with descriptive, narrative prose that builds atmosphere and emotion.",
                'poetic': "Respond with lyrical, poetic language that leans on metaphor and rhythm.",
            }

            system_prompt = (
                "You are DocumentGPT's AI co-writer. Continue the user's draft naturally, matching their tense, "
                "perspective, and voice. Output only the continuation‚Äîno preamble, no closing quotes."
            )
            if style in style_instructions:
                system_prompt += f" {style_instructions[style]}"

            context_window = context[-8000:]
            desired_words = max_tokens // 2
            human_prompt = (
                "Draft continuation request:\n"
                "---------------------------\n"
                f"{context_window}\n\n"
                f"Continue in the same format with roughly {desired_words}-{desired_words + 3} words."
            )

            try:
                response = llm.invoke(
                    [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=human_prompt),
                    ],
                    max_tokens=max_tokens,
                )
                completion = (response.content or "").strip().strip('"').strip("'")

                return {
                    'statusCode': 200,
                    'headers': headers,
                    'body': json.dumps({'completion': completion})
                }
            except Exception as err:
                print(f"‚ùå Autocomplete error: {err}")
                return {
                    'statusCode': 500,
                    'headers': headers,
                    'body': json.dumps({
                        'error': 'Failed to generate completion',
                        'detail': str(err)
                    })
                }

        # Documents endpoint
        if path == '/documents' and method == 'GET':
            user_id = event.get('queryStringParameters', {}).get('user_id')
            if not user_id:
                return {'statusCode': 400, 'headers': headers, 'body': json.dumps({'error': 'Missing user_id'})}
            
            docs_table = dynamodb.Table(DOC_TABLE)
            from boto3.dynamodb.conditions import Key
            resp = docs_table.query(
                KeyConditionExpression=Key('pk').eq(f'USER#{user_id}') & Key('sk').begins_with('DOC#')
            )
            
            documents = [{
                'doc_id': item.get('doc_id'),
                'filename': item.get('filename'),
                'summary': item.get('summary', ''),
                'questions': item.get('questions', []),
                'created_at': item.get('created_at')
            } for item in resp.get('Items', [])]
            
            return {'statusCode': 200, 'headers': headers, 'body': json.dumps({'documents': documents}, cls=DecimalEncoder)}
        
        # Usage endpoint
        if path == '/usage' and method == 'GET':
            return {'statusCode': 200, 'headers': headers, 'body': json.dumps({'plan': 'premium', 'chats_used': 0, 'limit': -1})}
        
        return {'statusCode': 404, 'headers': headers, 'body': json.dumps({'error': 'Not found'})}
    
    except Exception as e:
        print(f"‚ùå Error: {e!r}")
        traceback.print_exc()
        return {'statusCode': 500, 'headers': headers, 'body': json.dumps({'error': str(e)})}
